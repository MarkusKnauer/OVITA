For using experiments on ground_robot
Setup the Jackal with all the necessary packages from the link:
https://clearpathrobotics.com/assets/guides/foxy/jackal/JackalInstallRobotSoftware.html

Make sure that all topics are running and functional. 

Run the file waypoints.py in the package jackal_navigation

## Overview for Arm and installation of pose estimation etc.

The object pose estimation process is composed of the following stages:

### 1. Environment Setup (Simulation)
- A **tabletop environment** is created in **PyBullet** with multiple objects placed in a scene.
- A **third-person view RGB-D camera** is mounted to observe the entire scene from above or at an angle.

### 2. Image Acquisition
- The third-person camera captures:
  - **RGB image**
  - **Depth image**
- Camera intrinsic parameters are known and used for 3D reconstruction.

### 3. Language-Guided Segmentation
- The captured **RGB image** is processed using **LangSAM**:
  - **LangSAM** combines:
    - **CLIP** (ViT-L/14@336px) for vision-language embedding.
    - **Grounding-DINO** for **object detection** using natural language prompts.
    - **SAM 2.1 (Segment Anything Model)** for **precise segmentation**.
- The system is guided by **text prompts** corresponding to object names (e.g., "mug", "banana").
- The output is a **segmentation mask** for each specified object.

### 4. 3D Point Cloud Generation
- Each segmentation mask is combined with:
  - **Depth image**
  - **Camera intrinsics**
- A **3D point cloud** is generated by back-projecting the masked depth values into 3D space.
- These points are then **transformed into world coordinates** using the camera pose.

### 5. Pose Estimation
For each segmented object:
- **Center Position**:
  - Calculated using the **centroid** of the 3D point cloud.
- **Orientation (Quaternion)**:
  - **Principal Component Analysis (PCA)** is applied to the point cloud to extract the **principal axes**.
  - The dominant axis defines the orientation and is converted into a **quaternion**.
- **Dimensions**:
  - Estimated using the spread (extent) of the point cloud along each principal axis.

---

## Installation Guide

### 1. Install PyTorch (CUDA 12.4)
```bash
pip install torch==2.4.1 torchvision==0.19.1 --extra-index-url https://download.pytorch.org/whl/cu124

#### For Windows
```bash
pip install -U git+https://github.com/luca-medeiros/lang-segment-anything.git

## Pose Estimation: Step-by-Step Execution

This section guides you through the process of estimating object poses using a trajectory file.

---

### Steps to Run

#### 1. Load the Trajectory File
- Ensure your trajectory file is in **JSON format**.
- Example file name: `trajectory_0.json`

#### 2. Create a `SceneReconstructor` Object
- Import and instantiate the `SceneReconstructor` class.
- Provide the file path as an argument to the constructor.

#### 3. Run the Reconstructor
- Use the `.run()` method to begin processing.
- Optionally, set `use_gui=True` to enable the graphical interface.

#### 4. Retrieve Object Poses
- Use `.get_estimated_poses()` to retrieve the final object poses.

---

### Sample Python Script

```python
# Define the path to the trajectory JSON file
file_path = r"/<Path to File>/trajectory_0.json"

# Create a SceneReconstructor instance
reconstructor = SceneReconstructor(file_path)

# Run the reconstructor
reconstructor.run(use_gui=True)

# Retrieve estimated object poses
poses = reconstructor.get_estimated_poses()
print("Final Object Poses:", poses)


